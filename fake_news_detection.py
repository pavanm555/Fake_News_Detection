# -*- coding: utf-8 -*-
"""Fake News Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JYyxOXPhkNbI-gmqr6RlL2710G2zrj4e

# Fake News Detection
By: M PAVAN SAI RAMAKRISHNA REDDY
> pavansaim555@gmail.com

##Summary:
The given code is an implementation of fake news detection using machine learning. It involves various steps such as data cleaning, data preprocessing, feature extraction, and model training.

At first, the necessary libraries are imported such as numpy, pandas, regular expression (re), Natural Language Toolkit (nltk), stop words, Porter Stemmer, TfidfVectorizer, Logistic Regression, and accuracy score.

After importing libraries, the stop words in the English language are printed using print(stopwords.words('english')) to check the stop words which will be used later in the code.

Then the dataset is loaded using the pd.read_csv() method and the first five rows of the dataset are displayed using the head() method to get an initial overview of the dataset. The dataset is checked for null values using news_dataset.isnull().sum().

Then the data is cleaned using the stemming() function. In this function, regular expression is used to remove any characters other than the alphabets, then the text is converted to lowercase and split into words. Then the Porter Stemmer algorithm is used to convert the words into their root form, and finally, the stop words are removed from the text. The cleaned data is then assigned back to the original dataset's title column.

After cleaning the data, the data and label are separated using X = news_dataset['title'].values and Y = news_dataset['label'].values. The textual data is then converted to numerical data using TfidfVectorizer. This is done using the fit() and transform() methods of the vectorizer object. The dataset is split into training and testing data using train_test_split().

Finally, the Logistic Regression model is trained on the training data using the fit() method. The accuracy score of the training data and test data is calculated using the accuracy_score() method. The model is then used to predict the class of a new data point using model.predict().

If the predicted class is 'REAL', it is printed that the news is real, and if the predicted class is 'FAKE', it is printed that the news is fake.

Overall, the given code is a basic implementation of fake news detection using machine learning. It can be improved by using more advanced algorithms and techniques, and by using larger and more diverse datasets.

##Dataset Description
**news.csv**: A full training dataset with the following attributes:
  1.   Id: unique id for a news article
  2.   Title: the title ofa news article
  3.   Text: the text of the article; could be incomplete
  4.   Label: a label that marks the article as potentially unreliable

     *   Fake news
     *   Real news

Importing the Dependencies
"""

import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))

"""Data Pre-processing"""

# loading the dataset to a pandas DataFrame
news_dataset = pd.read_csv('/content/news.csv')

news_dataset.shape

# print the first 5 rows of the dataframe
news_dataset.head()

# counting the number of missing values in the dataset
news_dataset.isnull().sum()

# replacing the null values with empty string
news_dataset = news_dataset.fillna('')

print(news_dataset['title'])

# separating the data & label
X = news_dataset.drop(columns='label', axis=1)
Y = news_dataset['label']

print(X)
print(Y)

"""##Stemming:

Stemming is the process of reducing a word to its Root word

example:
actor, actress, acting --> act
"""

port_stem = PorterStemmer()

def stemming(title):
    stemmed_content = re.sub('[^a-zA-Z]',' ',title)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_dataset['title'] = news_dataset['title'].apply(stemming)

print(news_dataset['title'])

#separating the data and label
X = news_dataset['title'].values
Y = news_dataset['label'].values

print(X)

print(Y)

Y.shape

# converting the textual data to numerical data
vectorizer = TfidfVectorizer()
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

"""Splitting the dataset to training & test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)

"""Training the Model: Logistic Regression"""

model = LogisticRegression()

model.fit(X_train, Y_train)

"""Evaluation

accuracy score
"""

# accuracy score on the training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy)

"""Making a Predictive System"""

X_new = X_test[3]

prediction = model.predict(X_new)
print(prediction)

if (prediction[0]=='REAL'):
  print('The news is Real')
else:
  print('The news is Fake')

print(Y_test[3])

"""                               
                                                                                <-- THE END -->


"""